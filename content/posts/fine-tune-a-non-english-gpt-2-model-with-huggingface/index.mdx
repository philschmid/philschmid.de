---
title: Fine-tune a non-English GPT-2 Model with Huggingface
author: Philipp Schmid
date: 2020-09-06
image: ./cover.jpg
excerpt:
  Fine-tune non-English, German GPT-2 model with Huggingface on German recipes. Using their Trainer class and Pipeline
  objects.
tags:
  - NLP
  - GPT-2
  - Huggingface
links:
  colab: https://colab.research.google.com/github/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb
  github: https://github.com/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb
photograph: Photo by Peter Dawn on Unsplash
---

# introduction

Unless you‚Äôre living under a rock, you probably have heard about [OpenAI](https://openai.com/)'s GPT-3 language model.
You might also have seen all the crazy demos, where the model writes `JSX`, `HTML` code, or its capabilities in the area
of zero-shot / few-shot learning. [Simon O'Regan](https://twitter.com/Simon_O_Regan) wrote an
[article with excellent demos and projects built on top of GPT-3](https://towardsdatascience.com/gpt-3-demos-use-cases-implications-77f86e540dc1).

A Downside of GPT-3 is its 175 billion parameters, which results in a model size of around 350GB. For comparison, the
biggest implementation of the GPT-2 iteration has 1,5 billion parameters. This is less than 1/116 in size.

In fact, with close to 175B trainable parameters, GPT-3 is much bigger in terms of size in comparison to any other model
else out there. Here is a comparison of the number of parameters of recent popular NLP models, GPT-3 clearly stands out.

![model-comparison](./images/models.svg)

This is all magnificent, but you do not need 175 billion parameters to get good results in `text-generation`.

There are already tutorials on how to fine-tune GPT-2. But a lot of them are obsolete or outdated. In this tutorial, we
are going to use the `transformers` library by [Huggingface](https://huggingface.co/) in their newest version (3.1.0).
We will use the new `Trainer` class and fine-tune our GPT-2 Model with German recipes from
[chefkoch.de](http://chefkoch.de).

You can find everything we are doing in this
[colab notebook.](https://colab.research.google.com/github/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb)

---

## Transformers Library by [Huggingface](https://huggingface.co/)

![./images/transformers-logo](./images/transformers-logo.png)

The [Transformers library](https://github.com/huggingface/transformers) provides state-of-the-art machine learning
architectures like BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet, T5 for Natural Language Understanding (NLU), and
Natural Language Generation (NLG). It also provides thousands of pre-trained models in 100+ different languages and is
deeply interoperable between PyTorch & TensorFlow 2.0. It enables developers to fine-tune machine learning models for
different NLP-tasks like text classification, sentiment analysis, question-answering, or text generation.

---

# Tutorial

In the tutorial, we fine-tune a German GPT-2 from the [Huggingface model hub](https://huggingface.co/models). As data,
we use the [German Recipes Dataset](https://www.kaggle.com/sterby/german-recipes-dataset), which consists of 12190
german recipes with metadata crawled from [chefkoch.de](http://chefkoch.de/).

We will use the recipe Instructions to fine-tune our GPT-2 model and let us write recipes afterwards that we can cook.

![colab-snippet](./images/colab-snippet.png)

We use a Google Colab with a GPU runtime for this tutorial. If you are not sure how to use a GPU Runtime take a look
[here](https://www.philschmid.de/google-colab-the-free-gpu-tpu-jupyter-notebook-service).

**What are we going to do:**

- load the dataset from Kaggle
- prepare the dataset and build a `TextDataset`
- initialize `Trainer` with `TrainingArguments` and GPT-2 model
- train and save the model
- test the model

You can find everything we do in this
[colab notebook.](https://colab.research.google.com/github/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb)

---

# Load the dataset from Kaggle

As already mentioned in the introduction of the tutorial we use the
"[German Recipes Dataset](https://www.kaggle.com/sterby/german-recipes-dataset)" dataset from Kaggle. The dataset
consists of 12190 german recipes with metadata crawled from [chefkoch.de](http://chefkoch.de/). In this example, we only
use the Instructions of the recipes. We download the dataset by using the "Download" button and upload it to our colab
notebook since it only has a zipped size of 4,7MB.

![kaggle-dataset](./images/kaggle-dataset.png)

```python
#upload files to your colab environment
from google.colab import files
uploaded = files.upload()

#132879_316218_bundle_archive.zip(application/zip) - 4749666 bytes, last modified: 29.8.2020 - 100% done
#Saving 132879_316218_bundle_archive.zip to 132879_316218_bundle_archive.zip
```

After we uploaded the file we use `unzip` to extract the `recipes.json` .

```python
!unzip '132879_316218_bundle_archive.zip'

#Archive:  132879_316218_bundle_archive.zip
#inflating: recipes.json
```

_You also could use the `kaggle` CLI to download the dataset, but be aware you need your Kaggle credentials in the colab
notebook._

```python
kaggle datasets download -d sterby/german-recipes-dataset
```

here an example of a recipe.

```json
{
  "Url": "https://www.chefkoch.de/rezepte/2718181424631245/",
  "Instructions": "Vorab folgende Bemerkung: Alle Mengen sind Circa-Angaben und k√∂nnen nach Geschmack variiert werden!Das Gem√ºse putzen und in St√ºcke schneiden (die Tomaten brauchen nicht gesch√§lt zu werden!). Alle Zutaten werden im Mixer p√ºriert, das muss wegen der Mengen in mehreren Partien geschehen, und zu jeder Partie muss auch etwas von der Br√ºhe gegeben werden. Auch das Toastbrot wird mitp√ºriert, es dient der Bindung. Am Schluss l√§sst man das \u00d6l bei laufendem Mixer einflie\u00dfen. In einer gro\u00dfen Sch√ºssel alles gut verr√ºhren und f√ºr mindestens eine Stunde im K√ºhlschrank gut durchk√ºhlen lassen.Mit frischem Baguette an hei\u00dfen Tagen ein Hochgenuss.Tipps: Wer mag, kann in kleine W√ºrfel geschnittene Tomate, Gurke und Zwiebel separat dazu reichen.Die Suppe eignet sich hervorragend zum Einfrieren, so dass ich immer diese gro\u00dfe Menge zubereite, um den Arbeitsaufwand gering zu halten.",
  "Ingredients": [
    "1 kg Strauchtomate(n)",
    "1 Gem√ºsezwiebel(n)",
    "1 Salatgurke(n)",
    "1 Paprikaschote(n) nach Wahl",
    "6 Zehe/n Knoblauch",
    "1 Chilischote(n)",
    "15 EL Balsamico oder Weinessig",
    "6 EL Oliven√∂l",
    "4 Scheibe/n Toastbrot",
    "Salz und Pfeffer",
    "1 kl. Dose/n Tomate(n), gesch√§lte, oder 1 Pck. p√ºrierte Tomaten",
    "1/2Liter Br√ºhe, kalte"
  ],
  "Day": 1,
  "Name": "Pilz Stroganoff",
  "Year": 2017,
  "Month": "July",
  "Weekday": "Saturday"
}
```

# Prepare the dataset and build a `TextDataset`

The next step is to extract the instructions from all recipes and build a `TextDataset`. The `TextDataset` is a custom
implementation of the
[Pytroch `Dataset` class](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class) implemented
by the transformers library. If you want to know more about `Dataset` in Pytorch you can check out this
[youtube video](https://www.youtube.com/watch?v=PXOzkkB5eH0&ab_channel=PythonEngineer).

First, we split the `recipes.json` into a `train` and `test` section. Then we extract `Instructions` from the recipes
and write them into a `train_dataset.txt` and `test_dataset.txt`

```python
import re
import json
from sklearn.model_selection import train_test_split

with open('recipes.json') as f:
    data = json.load(f)

def build_text_files(data_json, dest_path):
    f = open(dest_path, 'w')
    data = ''
    for texts in data_json:
        summary = str(texts['Instructions']).strip()
        summary = re.sub(r"\s", " ", summary)
        data += summary + "  "
    f.write(data)

train, test = train_test_split(data,test_size=0.15)

build_text_files(train,'train_dataset.txt')
build_text_files(test,'test_dataset.txt')

print("Train dataset length: "+str(len(train)))
print("Test dataset length: "+ str(len(test)))

#Train dataset length: 10361
#Test dataset length: 1829
```

The next step is to download the tokenizer. We use the tokenizer from the `german-gpt2` model.

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("anonymous-german-nlp/german-gpt2")

train_path = 'train_dataset.txt'
test_path = 'test_dataset.txt'
```

Now we can build our `TextDataset`. Therefore we create a `TextDataset` instance with the `tokenizer` and the path to
our datasets. We also create our `data_collator`, which is used in training to form a batch from our dataset.

```python
from transformers import TextDataset,DataCollatorForLanguageModeling

def load_dataset(train_path,test_path,tokenizer):
    train_dataset = TextDataset(
          tokenizer=tokenizer,
          file_path=train_path,
          block_size=128)

    test_dataset = TextDataset(
          tokenizer=tokenizer,
          file_path=test_path,
          block_size=128)

    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer, mlm=False,
    )
    return train_dataset,test_dataset,data_collator

train_dataset,test_dataset,data_collator = load_dataset(train_path,test_path,tokenizer)
```

---

# Initialize `Trainer` with `TrainingArguments` and GPT-2 model

The¬†[Trainer](https://huggingface.co/transformers/main_classes/trainer.html#transformers.Trainer)¬†class provides an API
for feature-complete training. It is used in most of
the¬†[example scripts](https://huggingface.co/transformers/examples.html) from Huggingface. Before we can instantiate our
`Trainer` we need to download our GPT-2 model and create
¬†[TrainingArguments](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments). The
`TrainingArguments` are used to define the Hyperparameters, which we use in the training process like the
`learning_rate`, `num_train_epochs`, or `per_device_train_batch_size`. You can find a complete list
[here](https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments).

```python
from transformers import Trainer, TrainingArguments, AutoModelWithLMHead

model = AutoModelWithLMHead.from_pretrained("anonymous-german-nlp/german-gpt2")

training_args = TrainingArguments(
    output_dir="./gpt2-gerchef", #The output directory
    overwrite_output_dir=True, #overwrite the content of the output directory
    num_train_epochs=3, # number of training epochs
    per_device_train_batch_size=32, # batch size for training
    per_device_eval_batch_size=64,  # batch size for evaluation
    eval_steps = 400, # Number of update steps between two evaluations.
    save_steps=800, # after # steps model is saved
    warmup_steps=500,# number of warmup steps for learning rate scheduler
    )

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    prediction_loss_only=True,
)
```

---

# Train and Save the model

To train the model we can simply run `trainer.train()`.

```python
trainer.train()
```

After training is done you can save the model by calling `save_model()`. This will save the trained model to our
`output_dir` from our `TrainingArguments`.

```python
trainer.save_model()
```

---

# Test the model

To test the model we use another
[highlight of the transformers library](https://huggingface.co/transformers/main_classes/pipelines.html?highlight=pipelines)
called `pipeline`. [Pipelines](https://huggingface.co/transformers/main_classes/pipelines.html?highlight=pipelines) are
objects that offer a simple API dedicated to several tasks, `text-generation` amongst others.

```python
from transformers import pipeline

chef = pipeline('text-generation',model='./gpt2-gerchef', tokenizer='anonymous-german-nlp/german-gpt2',config={'max_length':800})

result = chef('Zuerst Tomaten')[0]['generated_text']

```

result:

"_Zuerst Tomaten dazu geben und 2 Minuten kochen lassen. Die Linsen ebenfalls in der Br√ºhe anbr√ºhen.Die Tomaten
auspressen. Mit der Butter verr√ºhren. Den Kohl sowie die Kartoffeln and√ºnsten, bis sie weich sind. "_

Well, thats it. We've done itüë®üèª‚Äçüç≥. We have successfully fine-tuned our gpt-2 model to write us recipes.

To improve our results we could train it longer and adjust our `TrainingArguments` or enlarge the dataset.

---

You can find everything in this
[colab notebook.](https://colab.research.google.com/github/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb)

Thanks for reading. If you have any questions, feel free to contact me or comment on this article. You can also connect
with me on [Twitter](https://twitter.com/_philschmid) or
[LinkedIn](https://www.linkedin.com/in/philipp-schmid-a6a2bb196/).
